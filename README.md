# ðŸš€ Task 1 - GPT-2 Fine-Tuning on Motivational Quotes

This is my submission for Task 1 of the **Generative AI Internship** at **Prodigy Infotech**.

## ðŸ“Œ Objective
Fine-tune OpenAIâ€™s GPT-2 model on a custom dataset of motivational quotes to generate positive, inspiring text.

## ðŸ”§ Tools Used
- Python ðŸ
- Hugging Face Transformers ðŸ¤—
- Google Colab
- PyTorch
- W&B (Weights & Biases)

## ðŸ“‚ Files
- https://colab.research.google.com/drive/1Y-JQYCf8NuADIzZByAOEdQXC-uUbwE41?usp=sharing â€“ Google Colab notebook with training and generation
- `motivational_quotes.txt`â€“ The text dataset used for fine-tuning

## ðŸ§  What I Learned
- How to tokenize custom text datasets
- How to fine-tune GPT-2 for specific writing styles
- Basics of NLP model training with Transformers
- Using generation parameters like temperature, top_k, and top_p

## ðŸ“Š Sample Output

> **Prompt:** Believe in yourself because  
> **Generated:** that's how you need to feel. Not knowing about it is something that can be incredibly powerful...

## ðŸ“Ž Status: âœ… Completed
