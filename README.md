# 🚀 Task 1 - GPT-2 Fine-Tuning on Motivational Quotes

This is my submission for Task 1 of the **Generative AI Internship** at **Prodigy Infotech**.

## 📌 Objective
Fine-tune OpenAI’s GPT-2 model on a custom dataset of motivational quotes to generate positive, inspiring text.

## 🔧 Tools Used
- Python 🐍
- Hugging Face Transformers 🤗
- Google Colab
- PyTorch
- W&B (Weights & Biases)

## 📂 Files
- https://colab.research.google.com/drive/1Y-JQYCf8NuADIzZByAOEdQXC-uUbwE41?usp=sharing – Google Colab notebook with training and generation
- `motivational_quotes.txt`– The text dataset used for fine-tuning

## 🧠 What I Learned
- How to tokenize custom text datasets
- How to fine-tune GPT-2 for specific writing styles
- Basics of NLP model training with Transformers
- Using generation parameters like temperature, top_k, and top_p

## 📊 Sample Output

> **Prompt:** Believe in yourself because  
> **Generated:** that's how you need to feel. Not knowing about it is something that can be incredibly powerful...

## 📎 Status: ✅ Completed
